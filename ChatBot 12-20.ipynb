{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 依存句法和语义依存分析\n",
    "http://www.shareditor.com/blogshow?blogId=77"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依存句法强调介词、助词等的划分作用，语义依存注重实词之间的逻辑关系\n",
    "\n",
    "依存句法就是句子成分之间一种依赖关系\n",
    "“张三昨天告诉李四一个秘密”，那么语义包括：谁告诉李四秘密的？张三。张三告诉谁一个秘密？李四。张三什么时候告诉的？昨天。张三告诉李四什么？秘密。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依存树：子节点依存于父节点\n",
    "\n",
    "依存投射树：实线表示依存联结关系，位置低的成分依存于位置高的成分，虚线为投射线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依存关系的五条公理  \n",
    "1. 一个句子中只有一个成分是独立的\n",
    "\n",
    "2. 其他成分直接依存于某一成分\n",
    "\n",
    "3. 任何一个成分都不能依存于两个或两个以上的成分\n",
    "\n",
    "4. 如果A成分直接依存于B成分，而C成分在句子中位于A和B之间，那么C或者直接依存于B，或者直接依存于A和B之间的某一成分\n",
    "\n",
    "5. 中心成分左右两面的其他成分相互不发生关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LTP依存关系标记\n",
    "主谓关系 SBV subject-verb 我送她一束花 (我 <-- 送)\n",
    "\n",
    "动宾关系 VOB 直接宾语，verb-object 我送她一束花 (送 --> 花)\n",
    "\n",
    "间宾关系 IOB 间接宾语，indirect-object 我送她一束花 (送 --> 她)\n",
    "\n",
    "前置宾语 FOB 前置宾语，fronting-object 他什么书都读 (书 <-- 读)\n",
    "\n",
    "兼语 DBL double 他请我吃饭 (请 --> 我)\n",
    "\n",
    "定中关系 ATT attribute 红苹果 (红 <-- 苹果)\n",
    "\n",
    "状中结构 ADV adverbial 非常美丽 (非常 <-- 美丽)\n",
    "\n",
    "动补结构 CMP complement 做完了作业 (做 --> 完)\n",
    "\n",
    "并列关系 COO coordinate 大山和大海 (大山 --> 大海)\n",
    "\n",
    "介宾关系 POB preposition-object 在贸易区内 (在 --> 内)\n",
    "\n",
    "左附加关系 LAD left adjunct 大山和大海 (和 <-- 大海)\n",
    "\n",
    "右附加关系 RAD right adjunct 孩子们 (孩子 --> 们)\n",
    "\n",
    "独立结构 IS independent structure 两个单句在结构上彼此独立\n",
    "\n",
    "核心关系 HED head 指整个句子的核心"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## https://www.ltp-cloud.com/document/\n",
    "url_get_base = \"http://api.ltp-cloud.com/analysis/\"\n",
    "args = { \n",
    "        'api_key' : 'l1N4o2G9290VdjWFVxkIwPZibU0OvlNPnxSojtCW',\n",
    "        'text' : '我是中国人。',\n",
    "        'pattern' : 'dp',\n",
    "        'format' : 'plain'\n",
    "    }\n",
    "result = urllib.request.urlopen(url_get_base, urllib.parse.urlencode(args).encode(encoding='UTF8')) # POST method\n",
    "content = result.read().strip()\n",
    "print (content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14  中文分词\n",
    "1）歧义消除；2）未登陆词识别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-最短路径分词法其实就是一元语法模型，每个词成为一元，独立存在，出现的概率可以基于大量语料统计得出，我们把一句话基于词表的各种切词结果都列出来，因为字字组合可能有很多种，所以有多个候选结果，这时我们利用每个词出现的概率相乘起来，得到的最终结果，谁最大谁就最有可能是正确的，这就是N-最短路径分词法。 \n",
    "\n",
    "基于n元语法模型的分词法就是在N-最短路径分词法基础上把一元模型扩展成n元模型，也就是统计出的概率不再是一个词的概率，而是基于前面n个词的条件概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jieba中文分词\n",
    "官方描述：\n",
    "\n",
    "基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)\n",
    "采用了动态规划查找最大概率路径, 找出基于词频的最大切分 组合\n",
    "对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法\n",
    "前两句话是说它是基于词表的分词，最后一句是说它也用了由字构词，所以它结合了两种分词方法\n",
    " \n",
    "\n",
    "### ik分词器\n",
    "基于词表的最短路径切词\n",
    "\n",
    " \n",
    "\n",
    "### ltp云平台分词\n",
    "主要基于机器学习框架并部分结合词表的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15 概率图模型\n",
    "http://www.shareditor.com/blogshow?blogId=81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成式模型一般用联合概率计算 代表是：n元语法模型、隐马尔可夫模型、朴素贝叶斯模型等  \n",
    "判别式模型一般用条件概率计算 代表是：最大熵模型、支持向量机、条件随机场、感知机模型等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**贝叶斯网络** 能够在已知有限的、不完整的、不确定信息条件下进行学习推理，所以广泛应用在故障诊断、维修决策、汉语自动分词、词义消歧等问题上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**马尔可夫模型** 还可以看成是一个关于时间t的状态转换过程，也就是随机的有限状态机，那么状态序列的概率可以通过计算形成该序列所有状态之间转移弧上的概率乘积得出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**隐马尔可夫模型** 广泛应用在词性标注、中文分词等，比如中文分词，最初你是不知道怎么分词的，前面的词分出来了，你才之后后面的边界在哪里，但是当你后面做了分词之后还要验证前面的分词是否正确，这样前后有依赖关系，而不确定中间状态的情况最适合用隐马尔可夫模型来解释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**条件随机场** 条件指的是随机变量的取值由一定的条件概率决定， 条件来自于我们有一些观察值，这是它区别于其他随机场的地方。特殊在给定观察序列X时某个特定的标记序列Y的概率是一个指数函数exp(∑λt+∑μs)，其中t是转移函数，s是状态函数，我们需要训练的是λ和μ。条件随机场主要应用在标注和切分有序数据上，尤其在自然语言处理、生物信息学、机器视觉、网络智能等方面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 17 让机器做词性自动标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**基于统计模型**\n",
    "\n",
    "HMM是一种基于条件概率的生成式模型，所以模型参数是生成概率，不妨假设每个词的生成概率就是它所有可能的词性个数的倒数。如果某个词在词表里没有呢？这时我们可以把它的生成概率初值设置为0。这就是隐马尔可夫模型参数初始化的技巧，总之原则就是用最小的成本和最接近最优解的目标来设定初值\n",
    "\n",
    "**基于规则**\n",
    "\n",
    "根据初始标注器标注出来的结果和人工标注的结果的差距，来生成一种修正标注的转换规则，这是一种错误驱动的学习方法。基于规则的方法好处在于：经过人工校总结出的大量有用信息可以补充和调整规则库，这是统计方法做不到的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计方法覆盖面比较广，新词老词通吃，常规非常规通吃，但对兼词、歧义等总是用经验判断，效果不好。规则方法对兼词、歧义识别比较擅长，但是规则总是覆盖不全。因此两者结合再好不过"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词性标注的校验\n",
    "\n",
    "检查词性标注的一致性。一致性指的是在所有标注的结果中，具有相同语境下同一个词的标注是否都相同  \n",
    "词性标注的自动校对。自动校对顾名思义就是不需要人参与，直接找出错误的标注并修正，这种方法更适用于一个词的词性标注通篇全错的情况，因为这种情况基于数据挖掘和规则学习方法来做判断会相对比较准确"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18 句法结构分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于概率上下文无关文法(PCFG)。基于PCFG分析需要有如下几个要素：终结符集合、非终结符集合、规则集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "终结符集合是：∑={我, 吃, 肉,……}，这个集合表示这三个字可以作为句法分析树的叶子节点  \n",
    "非终结符集合是：N={S, VP, ……}，这个集合表示树的非页子节点，也就是连接多个节点表达某种关系的节点  \n",
    "规则集：R={  \n",
    "NN->我    0.5  \n",
    "Vt->吃     1.0  \n",
    "NN->肉   0.5  \n",
    "VP->Vt NN  1.0  \n",
    "S->NN VP 1.0  \n",
    "……\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "有一种句法规则是：\n",
    "\n",
    "S——|\n",
    "|    |\n",
    "NN   VP\n",
    "     |——|\n",
    "     Vt   NN\n",
    "\n",
    "其中NN的位置可能是“我”，也可能是“肉”，是“我”的概率是0.5，是“肉”的概率是0.5，两个概率和必为1。\n",
    "其中Vt的位置一定是“吃”，也就是概率是1.0……。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "句法分析树生成算法是基于统计学习的原理，根据大量标注的语料库（树库），通过机器学习算法得出非终结符、终结符、规则集及其概率参数，然后利用动态规划算法生成每一句话的句法分析树，在句法分析树生成过程中如果遇到多种树结构，选择概率最大的那一种作为最佳句子结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19 语义消歧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**基于互信息的词义消歧方法**  \n",
    "中文“打人”对应英文“beat a man”，而中文“打酱油”对应英文“buy some sauce”。这样就知道当上下文语境里有“人”的时候“打”的含义是beat，当上下文语境里有“酱油”的时候“打”的含义是buy。按照这种思路，基于大量中英文对照的语料库训练出来的模型就可以用来做词义消歧了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**基于贝叶斯分类器的消歧方法**  \n",
    "语境(context)记作c，语义(semantic)记作s，多义词(word)记作w，那么要计算的就是多义词w在语境c下具有语义s的概率p(s|c)  \n",
    "根据贝叶斯公式： p(s|c) = p(c|s)p(s)/p(c)  \n",
    "因为p(c)是既定的，所以只考虑分子的最大值：s的估计=max(p(c|s)p(s))  \n",
    "语境c在自然语言处理中必须通过词来表达，也就是由多个v(词)组成 max(p(s)∏p(v|s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 语义角色标注\n",
    "http://www.shareditor.com/blogshow?blogId=89"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于短语结构树的语义角色标注方法、  \n",
    "基于浅层句法分析结果的语义角色标注方法、  \n",
    "基于依存句法分析结果的语义角色标注方法  \n",
    "\n",
    "句法分析->候选论元剪除->论元识别->论元标注->语义角色标注结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
